---
title: "Case Study: Predicting the outcomes of the 2017 Dutch General Elections"

author: "Ilse van Beelen, Floor Komen"


affiliation: Leiden University
date: "`r format(Sys.time(), '%B %d, %Y')`"
header-includes:
   - \usepackage{float}
output:
  pdf_document:
    citation_package: natbib
    fig_caption: yes
    keep_tex: yes
  html_document:
    df_print: paged
fontfamily: mathpazo
fontsize: 11pt
keywords: 'put some keywords here '
endnote: no
abstract: Put the abstract over here
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy= TRUE, tidy.opts = list(comment = FALSE),
  dev.args = list(bg = 'transparent'),
  fig.align = "center",
  fig.pos = "H")


library("tidyverse")
library("car")
library(reshape2)
library(lindia)
library("GGally")
library("Hmisc")
library("lattice")
library("survival")
library("formatR")
#library("kableExtra")
library("knitr")
library(faraway)

#options(kableExtra.latex.load_packages = FALSE)

rm(list = ls()) # empty work space
Data_CDA <- read.csv("1_clean_data/Clean_data_CDA_2018-12-21.csv",
                 stringsAsFactors=F, header = T)

# Remove Non_west for now. We later add it again
Data_CDA <- Data_CDA %>%
  select(-Non_west)

```

# 1. Introduction 


## 1.1 Motivation

For this case study, it was decided to combine the outcome from the Dutch elections of 2017 and demographic data. Both are collected per municipality and are well maintained and reliable. This makes  A lot of information is available for both in the Netherlands. This will hopefully result in observing voting trends per demographic group. The final goal is to validate the model for making futyre predictions.

![Party landscape](Partijlandschap.jpg){width=250px}
**Dutch political parties**   
This figure displays the diffences between the political parties in the Netherlands. The Netherlands has a total of 13 parties. This investigation focusses on only one party. This party should not be to extreme left/right/conservative/progressive and should also be one of the bigger parties. Otherwise, there is not enough data available, making the results less reliable. Therefore, party CDA is chosen. 

In this research the above described demographics are chosen because of their influence on a municipality level. The expectation is that a municipality with more non-western residents for example votes different than a municipality with less non-western residents. This is the same for the other two demographics. Other demographics are also researched, for example gender, but on a municipality level there is no large difference between the amount of men and women per municipality. So that is a more interesting demographic to research on an individual level. 
*The standardized income per municipality* are given in thousands. *the urban index of a municipality* is a database with five categories per municipality. These five categories are: 

\begin{itemize}
\item Really strong urbanity (more than 2500 addresses per $km^2$) 
\item Strong urbanity (1500-2500 addresses per $km^2$) 
\item Moderate urbanity (1000- 1500 addresses per $km^2$) 
\item Little urbanity (500-1000 addresses per $km^2$)  
\item No urbanity (less than 500 addresses per $km^2$)
\end{itemize}

Per municipality the amount of $km^2$ per category is given. The *non-west residents per municipality* is given in an amount per municipality, also the total amount of residents is given per municipality. 


## 1.2 Data sources

**Electoral data**
For the electoral data, the results of the 2017 general election are used. This is the most recent national election and is of the most important election type in the Netherlands. Furthermore, it had a turnup of 81.9%. Therefore, it seems plausible that the data for this election is representative of the political makeup of different municipalities. We downloaded the raw data directly from the official government source.^[https://data.overheid.nl/data/dataset/verkiezingsuitslag-tweede-kamer-2017] This contained a .csv file with the raw number of votes for every party in every municipality. 


**Demographical data**  
We got our demographical data from the CBS, the official Dutch statistical agency.^[https://opendata.cbs.nl/statline/#/CBS/nl/dataset/70072ned/table?ts=1544803364892] From the wealth of demographical information available we picked a handful of attributes that we suspected (based on prior research and some gut feeling) to be useful as predictor variables. We landed on five demographical attributes: education grade, average income, age, urbanization and the amount of people with a non-western background. Note that the data we downloaded from the CBS site usually had to be transformed to get it in a useful predictor variable format. The specifics of these are described in the next section.


## 1.3 Data cleaning 

An extensive amount of data cleaning had to be done. Below these steps are describes and a small part of code is displayed.

**Electoral data**  



**Demographical data**  


The variable *non-western residents* are divided in three groups:

\begin{itemize}
\item Municipalities with less than 5 % non-western residents 
\item Municipalities with 5-10 % non-western resident 
\item Municipalities with mre than 10 % non-western residents
\end{itemize}

```{r}
Data_CDA$Non_west <- ifelse(Data_CDA$Non_west_frac < 0.05, 1, NA)
Data_CDA$Non_west <- ifelse(Data_CDA$Non_west_frac >= 0.05 
                        & Data_CDA$Non_west_frac < 0.1, 2, Data_CDA$Non_west)
Data_CDA$Non_west <- ifelse(Data_CDA$Non_west_frac >= 0.1, 3, Data_CDA$Non_west)
Data_CDA$Non_west <- as.factor(Data_CDA$Non_west)


```

At last, the electoral data and demographic data are combined again. Only the municipality Boxmeer is removed, due to a mistake not all the votes are reported here^[https://www.gelderlander.nl/boxmeer/7-600-stemmen-in-boxmeer-niet-meegenomen-in-uitslag-verkiezingen~a063ee9e/]. The final dataset has no NAs

```{r}
summary(Data_CDA)
```



## 1.3 Data visualisation

In this part the cleaned data is visualized, so that a good picture can be obtained of the current data. First of all some demographics of data will be showed. In figure \ref{1} of the *parties*, *the urban index*, *the percentage of highly educated residents*, *the mean income*, *The non west residents factor* and * the percentage 60 plus* are plotted. 
As you can see in the plot, they are normal distributed. Because of the low values at the x-axis, the CDA, GroenLinks, 60 plus percentage and the highly educated densities are above 1. The area beneath the curve sums to 1, so it is correct.

```{r demographics_data1, echo=FALSE}
#knitr::kable(
#  summary(Data_CDA[,-1], digits=2), caption="Data summary", "latex",booktabs = T) %>% kable_styling(latex_options=c("scale_down", "striped"), bootstrap_options = "hover")
```

```{r demographics_data, echo=FALSE, warning=FALSE,message=FALSE,error=FALSE, fig.cap="\\label{1} Density plot" , fig.width=5, fig.height=3}
#### Demographics of data ####

dens = ggplot(melt(Data_CDA[,c(2:9)]), aes(x = value)) + 
  facet_wrap(~ variable, scales = "free", ncol = 2) + 
  geom_histogram(aes(y=..density..), position="identity", alpha=0.5)+
  geom_density(alpha=0.4, aes(fill = "red", col = "red"))+
  theme(legend.position="none")

plot(dens)
```


**Correlation heatmap**
In this heatmap (figure \ref{2}) the correlation between explanatory and respons variable are showed. The red color means a positive relation, the purple color means a negative relation. The non_west variable is not taken into account, because it is a factor and the other variables are continous. 


```{r correlation_heatmap, echo=FALSE, warning=FALSE,message=FALSE,error=FALSE, fig.cap= '\\label{2}Correlation between explanatory and respons variables', fig.width=5, fig.height=3}
# Heatmap of the correlations
heatmap = ggcorr(Data_CDA[,c(2:9)], 
                 low = "darkblue", mid = "lightyellow", high = "red",
                 label = T, label_size = 2.5, label_round = 3,   
                 color = 'black', size = 4, layout.exp = 2, hjust = 1) 

plot(heatmap)

```

```{r  echo=FALSE, warning=FALSE,message=FALSE,error=FALSE, fig.cap= "Votes for CDA per municipality", fig.width=5, fig.height=3}
p1 <- ggplot(Data_CDA, aes(x = Non_west, y = CDA_frac, fill = Non_west)) + 
       geom_boxplot(outlier.colour="black", 
                    outlier.size=2, 
                    outlier.fill =  "red",
                    na.rm = F,
                    color = "black") +
       xlab("Non-western residents") +
       ylab("Votes for CDA") +
       scale_x_discrete(labels = c("< 5%", "5-10%", "> 10 %")) +
       guides(fill = F)

#plot(p1)
```


```{r  echo=FALSE, warning=FALSE,message=FALSE,error=FALSE, fig.cap= "Urbanity index against non-western residents" , fig.width=5, fig.height=3}

p2 <- ggplot(Data_CDA, 
             aes(x = Non_west, y = Urban_index, fill = Non_west)) + 
  geom_boxplot(outlier.colour="black", 
               outlier.size=2, 
               outlier.fill =  "red",
               na.rm = F,
               color ="black") +
  #ggtitle("Urbanity index against non-western residents") +
  xlab("Non-western residents") +
  ylab("Urbanity index") +
  ylim(c(0,4)) +
  scale_x_discrete(labels = c("< 5%", "5-10%", "> 10 %")) +
  guides(fill = F)
 

#plot(p2)
```




```{r boxplots_code, echo=FALSE, warning=FALSE,message=FALSE,error=FALSE}
# Linear regression between votes for CDA and mean income
Euro <- "\u20AC" # euro sign

p3 <- ggplot(Data_CDA, aes(x=Mean_income, y=CDA_frac)) + 
  geom_point() + 
  geom_smooth(method=lm, se = T) +
  xlab(paste("Mean income per municipality ")) +
  ylab("Votes for CDA") +
  ylim(c(0,0.45)) +
  xlim(c(20,42)) 

#plot(p3) 


p4 <- ggplot(Data_CDA, aes(x=Mean_income, y=High_educated_frac)) + 
  geom_point() + 
  geom_smooth(method=lm, se = T) + # standard error = True
  ##ggtitle("Highly educated residents and mean income per municipality ") +
  xlab(paste("Mean income (x ",Euro, " 1000)", sep = "")) +
  ylab("Highly educated residents") +
  xlim(c(20,42)) 

#plot(p4)

# Urbanity index and highly educated residents
p5 <- ggplot(Data_CDA, aes(x=Urban_index, y=High_educated_frac)) + 
  geom_point() + 
  geom_smooth(method=lm, se = T) + # standard error = True
  ##ggtitle("Highly educated residents and urbanity index") +
  xlab("Urbanity index") +
  ylab("Highly educated residents") +
  xlim(c(0,4))


#plot(p5)


# Urbanity index and CDA votes
p6 <- ggplot(Data_CDA, aes(x=Urban_index, y=CDA_frac)) + 
  geom_point() + 
  geom_smooth(method=lm, se = T) + 
  xlab("Urbanity index") +
  ylab("Votes for CDA") +
  xlim(c(0,4.1)) +
  ylim(c(0,0.45))



p9 <- ggplot(Data_CDA, aes(x=Urban_index, y=Frac_60plus)) + 
  geom_point() + 
  geom_smooth(method=lm, se = T)  +
  xlab("Urbanity index") +
  ylab("> 60 years old residents") +
  xlim(c(0, 4)) +
  ylim(c(0,0.20))# + 


```


```{r, include=FALSE}

# Function to plot multiple plots together
multiplot <- function(..., plotlist=NULL, file, cols=2, layout=NULL) {
  library(grid)
  
  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)
  
  numPlots = length(plots)
  
  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                     ncol = cols, nrow = ceiling(numPlots/cols))
  }
  
  if (numPlots==1) {
    print(plots[[1]])
    
  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
    
    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
      
      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```

**Multilineair plots CDA **
In these two plots you can see a scatterplot with on the y-axis the votes for CDA in percentages and on the x-axis on the left graph the mean income per municipality in 1000 euro. The right plot has the urbanity index as x-axis. As you can see, the trend is that when the mean income goes up, the votes for CDA goes down. Same with the urbanity index. In the model formulation graph these trends are checked. 
```{r, echo=FALSE, warning=FALSE,message=FALSE,error=FALSE, fig.cap= "\\label{3}Scatterplots CDA", fig.width=5, fig.height=3}
multi_linearCDA <- multiplot(p3, p6)

```



**Exploratory plots of variables**
These three plots are scatterplots of the explanatory variables. 
```{r, echo=FALSE, warning=FALSE,message=FALSE,error=FALSE, fig.width=5, fig.height=3, fig.cap= "\\label{5}Scatterplot explanatory variables"}
multi_linear <- multiplot(p4, p5)
plot(p9)

```

**Multiple boxplots**
In this graph boxplots are made, to compare some variables. A boxplot is a standardized way to display the distribution of data. It gives the minimum, first quartile, median, third quartile and the maximum. If there are any outliers, the boxplot is extended with those. The line within the box is the median, the first and third quartile are the down- and upside of the box, respectively. The length of the box is the Inter Quartile Range (IQR). The minimum and maximum are 1.5X Inter Quartile Range (IQR). Observations further away can be considered outliers.

```{r, echo=FALSE, warning=FALSE,message=FALSE,error=FALSE, fig.width=5, fig.height=3, fig.cap= "\\label{6}Three boxplots: Votes for CDA, Votes for GroenLinks and Urbanity index"}

multiplot(p1, p2)

```

# 2. Multiple linear regression
In this chapter multiple linear models are generated. The demographics tested in this model are the highly educated fraction in a municipality `High_educated_frac`, the urban index of a municipality `Urban_index`, the mean income of the municipality `Mean_income`, the non-west factor `Non_west` and the fraction that is 60 plus in the municipality `Frac_60plus`. 
The error assumptions are also discussed. This are assumptions made for the residuals, to check if meet the requirements for correct linear regressions. These assumptions are: 
* Linearity: The expected value of the error is zero
* Constant variance: The variance of the error is constant 
* Normality: The errors are normally distributed
* Indepence: The observations are sampled indipendently


## First model
 The first model will be the model with all the demographics:  
$Y_i = \beta_0 + 	\beta_1*high educated fraction + \beta_2*Urban index + \beta_3*Mean income + \beta_4*Non west2 +\beta_5*Non west3 + \beta_6*Frac 60plus + \epsilon i$   
The outcome of this model is shown below: 
\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr() \\ 
  \hline
(Intercept) & 0.3381 & 0.0314 & 10.78 & 0.0000 \\ 
  High\_educated\_frac & -0.0864 & 0.0454 & -1.90 & 0.0576 \\ 
  Urban\_index & -0.0193 & 0.0041 & -4.69 & 0.0000 \\ 
  Mean\_income & -0.0015 & 0.0011 & -1.46 & 0.1453 \\ 
  Non\_west2 & -0.0223 & 0.0065 & -3.45 & 0.0006 \\ 
  Non\_west3 & -0.0455 & 0.0095 & -4.77 & 0.0000 \\ 
  Frac\_60plus & -0.5904 & 0.1494 & -3.95 & 0.0001 \\ 
   \hline
\end{tabular}
\end{table}

The first model is the total model, `high_educated_frac` and `Mean_income` do not have a significant t-value. Before any conclusions are made, the assumptions are checked via plots and the VIF is checked. The VIF is the Variation Inflation Factor, it implies if there is multicollinearity between two or more variables. The formula for VIF is $1/(1-R^2)$ and the thresholdvalue is 10. So values above 10 give signs of multicollinearity. As shown below none of the values are above 10, so no signs of collinearity. 
```{r LM_VIFcheck, warning=FALSE,message=FALSE,error=FALSE, echo=FALSE}
lm_CDA_1 <- lm(CDA_frac ~ High_educated_frac +  Urban_index+
               Mean_income +Non_west + Frac_60plus,data = Data_CDA)
vif(lm_CDA_1)
#xtable(summary(lm_CDA_1))
```
```{r, echo=FALSE, warning=FALSE,message=FALSE,error=FALSE, fig.width=12, fig.height=3, fig.cap= "\\label{afm}assumptions first model"}
par(mfrow = c(1,4))
plot(lm_CDA_1, which = 1, id.n = 5) # we see heterogenitiy of variance
qqPlot(lm_CDA_1) # right tail is skewed (non normality)

plot(lm_CDA_1, which = 3, id.n = 5) # var of error not equal, non-linearity, obs 300 & 76 outliers
plot(lm_CDA_1, which = 4, id.n = 5)# cutt off val: 0.011. Everything above cut-off is at least influencal point, maybe outlier
abline(h = 0.011, col = "red")

```

In figure \ref{afm} the four plots are shown. The first plot (Residuals vs Fitted) shows that the residuals have a 'loudspeaker pattern', the variance of the residuals tends to increase with an increase of the fitted value. Because of this, a BoxCox graph is consulted. This graph suggests a transformation for the response. The BoxCos figure \ref{BC1} in has a 95% Confidence interval located around the 0. So a ln transformation is suggested. 
```{r, echo=FALSE, warning=FALSE,message=FALSE,error=FALSE, fig.width=5, fig.height=3, fig.cap= "\\label{BC1}BoxCox first model"}
gg_boxcox(lm_CDA_1)

```
## Second model
In the second model the response variable will be ln transformed. So the new model will be:  
$ln(Y_i) = \beta_0 + 	\beta_1*high educated fraction + \beta_2*Urban index + \beta_3*Mean income + \beta_4*Non west2 + \beta_5*Non west 3 + \beta_6*Frac 60plus + \epsilon i$ 
\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr() \\ 
  \hline
(Intercept) & -0.9944 & 0.1882 & -5.28 & 0.0000 \\ 
  High\_educated\_frac & -0.8808 & 0.2723 & -3.24 & 0.0013 \\ 
  Urban\_index & -0.1388 & 0.0247 & -5.62 & 0.0000 \\ 
  Mean\_income & -0.0024 & 0.0064 & -0.38 & 0.7042 \\ 
  Non\_west2 & -0.0991 & 0.0389 & -2.55 & 0.0112 \\ 
  Non\_west3 & -0.2763 & 0.0572 & -4.83 & 0.0000 \\ 
  Frac\_60plus & -2.6940 & 0.8965 & -3.01 & 0.0028 \\ 
   \hline
\end{tabular}
\end{table}


```{r, echo=FALSE, warning=FALSE,message=FALSE,error=FALSE, fig.width=12, fig.height=3, fig.cap= "\\label{asm}assumptions second model"}
lm_CDA_2 <- lm(log(CDA_frac) ~ High_educated_frac +  Urban_index+
                 Mean_income +Non_west + Frac_60plus,data = Data_CDA)
#xtable(summary(lm_CDA_2))
# Check assumptions
par(mfrow = c(1,4))
plot(lm_CDA_2, which = 1) # still heteroscadacity, 
qqPlot(lm_CDA_2) # normally distributed
plot(lm_CDA_2, which = 3) # both the fitted and residuals are more spread out
plot(lm_CDA_2, which = 4)  # only obs 16(amsterdam) is influential. 
# Cutoff val for Cooks: 4 / (369 - 5 - 1) = 0.011
```
The plots in figure \ref{asm} show one big outlier, the municipality Amsterdam which has number 16. Amsterdams value for the cooks distance goes way above the cutoff value for cooks, $4/(369-5-1)=0.011$. It is also outside the (-3,3) range with the studentized residuals. That is why this municipality is removed.   
For the second model without Amsterdam, a step function is used. This step function uses the AIC for backward elimination. If the AIC can get lower, because a variable is removed that variable will be removed else no variable is removed.  The formula for AIC is $AIC=-2log(likelihood)+2p$, p is the number of parameters in the model. The variables that are left are the variables used in the final model. 

```{r, echo=FALSE, warning=FALSE,message=FALSE,error=FALSE}
lm_CDA_2a <-lm(log(CDA_frac) ~ High_educated_frac +  Urban_index+
                 Mean_income +Non_west + Frac_60plus,data = Data_CDA[-16,])
step(lm_CDA_2a)
```

## Final model
The backward elimination gave the final model.  
$ln(Y_i) = \beta_0 + 	\beta_1*high educated fraction + \beta_2*Urban index +  \beta_4*Non west2 + \beta_5*Non west 3 + \beta_6*Frac 60plus + \epsilon i$ 
The coëfficients are given in the table below
\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & -1.0298 & 0.1365 & -7.54 & 0.0000 \\ 
  High\_educated\_frac & -0.8277 & 0.2129 & -3.89 & 0.0001 \\ 
  Urban\_index & -0.1311 & 0.0240 & -5.46 & 0.0000 \\ 
  Non\_west2 & -0.1141 & 0.0378 & -3.02 & 0.0027 \\ 
  Non\_west3 & -0.2871 & 0.0559 & -5.13 & 0.0000 \\ 
  Frac\_60plus & -3.0168 & 0.8799 & -3.43 & 0.0007 \\ 
   \hline
\end{tabular}
\end{table}

First, the error-assumptions will be checked (figure \ref{afm}). The 'loudspeaker pattern' almost disappeared, so the variance of the error is almost stable. Because the model is already transformed into a log model, there is not much transformation possible to let the pattern totally disappear. The influence will not be that high, because the pattern is relatively small.  In the qq-plot it is visible that almost all municipalities are in the -3,3 range. Only the municipalities Oostzaan and Tubbergen are outside this range. This is because Oostzaan has an extreme low CDA_frac(0.067) and Tubbergen an extreme high CDA_frac(0.42). The decision is made not to delete these values, because they are still in the 95% envelope of the qq-plot. In the third plot the red line is notable, it goes up at the end of the graph. This means that there is some non-constant error variance, but because the scatter is not that big no action is needed. 

```{r, echo=FALSE, warning=FALSE,message=FALSE,error=FALSE, fig.width=12, fig.height=3, fig.cap= "\\label{afm}Assumptions final model"}
lm_CDA_final <-lm(log(CDA_frac) ~ High_educated_frac +  Urban_index +Non_west + Frac_60plus,data = Data_CDA[-c(16),])
#xtable(summary(lm_CDA_final))
# Check assumptions
par(mfrow = c(1,4))
plot(lm_CDA_final, which = 1) # still heteroscadacity, 
qqPlot(lm_CDA_final) # normally distributed
plot(lm_CDA_final, which = 3) # both the fitted and residuals are more spread out
plot(lm_CDA_final, which = 4)  # only obs 16(amsterdam) is influential. 
# Cutoff val for Cooks: 4 / (369 - 5 - 1) = 0.011
```


The estimates for the predictors are filled in the model and the following results are obtained:   

$ln(Y_i) = -1.0298  	-0.8277*high educated fraction  -0.1311*Urban index   -0.1141*Non west2  -0.2871*Non west3 -3.0168*Frac 60plus + \epsilon i$  

Something notable is that all coëfficients are negative, but because the fitted values are logaritmic the eventual output will be positive. The added-variable plots are visible below (figure\{avf}). 

```{r, echo=FALSE, warning=FALSE,message=FALSE,error=FALSE, fig.width=10, fig.height=5, fig.cap= "\\label{avf} Added variable plots final model"}
avPlots(lm_CDA_final)
```


```{r, echo=FALSE, warning=FALSE,message=FALSE,error=FALSE, fig.width=10, fig.height=5, fig.cap= "\\label{pfm} plot final High educated frac"}
function_High_educated_frac <- function(x){
  exp(-1.0298) + exp(-0.8277)*x +exp(-0.1311)*1.4280 +exp(-3.0168)*0.1327
}
function_Urban_index <- function(x){
  exp(-1.0298) + exp(-0.8277)*0.266 +exp(-0.1311)*x +exp(-3.0168)*0.1327
}


```


## Cross validation

```{r}
K <- 5
index <- rep(1:K, floor(nrow(Data_CDA)/K)+1)[1:nrow(Data_CDA)]
summary(as.factor(index))
fold.index <- sample(index)

Loss <- function(x, y){
  sum((x-y)^2)/length(x)
}
loss <- numeric(K)

for (k in 1:K){
  training <- Data_CDA[fold.index!=k, ]
  validation <- Data_CDA[fold.index==k, ]
  training.fit <- lm_CDA_final
  validation.predict <- predict(training.fit, newdata=validation, type='response')
  loss[k] <- Loss(log(validation$CDA_frac), validation.predict)
}

#average, with weights equal to the number of objects used to calculate the loss at each fold:
mean(loss)
```



# 3. Logistic regression

The raw respons variable is the absolute amount of residents per municipality that voted for CDA. For linear regresssion, this variable is transformed to a fraction. However, the absolute total amount of votes per municipality is also available. Therefore, a binomial model would be a better fit to the data. A second model is developed that uses the logit as link function to transform the range of the respons. The choice for the logit was easily made. Because the inverse of the logit is directly interpretable as the log-odds ratio. This link displays the underlaying pattern of this data best. Below, the formula for the link function:

$\eta = log(\frac{\theta}{1 - \theta}) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_n x_n$

Where $\theta$ is the probability of votes for CDA.

Also for logistics regression are diagnostic plots needed to visualise the deviance/pearson residuals and search for outliers. Most of the diagnostics from the linear model extend relatively straighforward to logistic regression. However, leverages are no longer just a function of the explanatory variable, but also depend on the respons due to iterated weighted least squares. Furthermore, $\theta$ can never be zero or one. Fortunately, this was not the case for any of the observations in this dataset.

## First model

Again, the first is the full model. Stepwise backward elimination is used to find the optimal model. Below, the formula for the full model:

$log(\frac{\theta}{1 - \theta}) = \beta_0 + \beta_1 \cdot UrbanIndex + \beta_2 \cdot HighlyEducatedFraction + \beta_3 \cdot MeanIncome + \beta_4 \cdot NonWest + \beta_5 \cdot Fraction60Plus$

```{r}
glm_CDA_1 <- glm(cbind(CDA_abs, Total_abs - CDA_abs) ~ Urban_index + 
                   High_educated_frac + Mean_income +Non_west + Frac_60plus, 
                 family=binomial(link = "logit"), data = Data_CDA)


summary(glm_CDA_1)
```

The summary visualizes that the all the variables are very significant, with small errors. The full model has 359 degrees of freedom and it is expected that the residual deviance is roughly equivalent. However, the residual deviance is far above this value. These are strong indications that this model suffers from overdispersion. This assumption seems reasonable, because there is a very large variance in how many residents per municapality voted for CDA. In some municipalities only 3% voted for CDA, while it others nearly 50% voted for CDA. It is concluded that a quasi-binomial would fit the data better.


## Second model

```{r}
glm_CDA_2 <- glm(cbind(CDA_abs, Total_abs - CDA_abs) ~ Urban_index + 
                   High_educated_frac + Mean_income + Non_west + Frac_60plus, 
               family=quasibinomial(link = "logit"), data = Data_CDA)

summary(glm_CDA_2)
```

By applying a quasi binomial model, a dispersion parameter $\phi$ is included, resulting in larger standard errors and less significant p-values. $\phi$ is estimated on the data at 254.0441.  The variables `Frac_60plus`, `Mean_income` and factor `Non_west2` are no longer significant.

No goodness of fit test is possible because of the free dispersion parameter.The decision to remove variables is done based on the lowest F-test.

```{r}
vif(glm_CDA_2)
drop1(glm_CDA_2, test = "F")
```
According to the F-test `Frac_60plus` should be removed. This variable has a F-value of 1.32 and a corresponding p-value of 0.25. The values for the VIF are all very low, meaning there is barely collinearity between the explanatory variables.

At last, the residuals and cook's distance are visualized
```{r}
par(mfrow = c(1,3))
halfnorm(residuals(glm_CDA_2,"pearson"))
plot(glm_CDA_2, which = 1, id.n = 5)
plot(glm_CDA_2, which = 4, id.n = 5)

Data_CDA[c(16,74, 265),]
```

The left plot display the half-normal quantiles against the pearson residuals. Ideally, these residuals would not be greater than 3. However, this plot shows residuals even up to 80. The middle plot displays the predicted values against the deviance resiuals. Also here a large spread of the residuals is observed. There seems to be non-constant erro variance, because the spread becomes larger for higher predicted values. The right plots shows the cook's distance, which can identify influential observations. Observation 16 is an outlier, because it is very influential and stands out from any pattern in the residual plots. Furthermore, Dinkelland (obs 74) and Rotterdam (obs 265) are also influential. Amsterdam is the municipality with the lowest percentage of CDA votes and Dinkelland has the highest percentage of CDA votes. 


```{r}
avPlots(glm_CDA_2)
```

The avPlots help to interpret the partial regression coefficients when the other variables are held constant.The partial regression line is highly influenced by observation 16 again. The blue lines do not represent the data well at the moment.

## Third model

For this model the variable `Frac_60plus` is removed, because it had the lowest F-value. Furthermore, the observations 16 (Amsterdam) and 265 (Dinkelland) are removed. These influenced the partial regression coefficients greatly and had large residual and cook's distances. These steps were originally done in two, but they are combined for this report.

```{r}
glm_CDA_4 <- glm(cbind(CDA_abs, Total_abs - CDA_abs) ~ Urban_index + 
                   High_educated_frac + Mean_income + Non_west, 
               family=quasibinomial(link = "logit"), data = Data_CDA[-c(16, 265),])

summary(glm_CDA_4)
```
By removing observation 16 and 265, the factor `Non_west2` has become significant. Also the residual deviance has decreased slightly, but is still far above the degrees of freedom.


```{r}
par(mfrow = c(1,3))
halfnorm(residuals(glm_CDA_4,"pearson"))
plot(glm_CDA_4, which = 1, id.n = 5)
plot(glm_CDA_4, which = 4, id.n = 5)
```

The plot left still displays very large pearson residuals. The plot in the middle seems homogenic distributed, which slightly more variance for higher predicted values. And the cook's distance no longer displays highly influential observations.

```{r}
drop1(glm_CDA_4, test = "F")
```
According to the F-test `Mean_income` should be removed as well, because the F-value is below 1 and the corresponding p-value is 0.96.

## Final model

The final model is reached after dropping the variables `Mean_income` and `Frac_60plus`. It's formula is as follows:

$logit(p_{ij}) = -1.09  -0.14 \cdot UrbanIndex -1.23 \cdot HighlyEducatedFraction - 0.12 \cdot NonWest:2 - 0.34 \cdot NonWest:3$

```{r}
glm_CDA_final <- glm(cbind(CDA_abs, Total_abs - CDA_abs) ~ Urban_index  + 
                   High_educated_frac + Non_west, 
                 family=quasibinomial(link = "logit"), 
                 data = Data_CDA[-c(16, 265),])

summary(glm_CDA_final)
drop1(glm_CDA_final, test = "F")
```
The F-test shows that all variables now significantly contribute to the model. The residual deviance has slightly decreased, in compare to model `glm_CDA_2` with all the variables. However, the residual deviance is still much larger then the degrees of freedom.


```{r}
par(mfrow = c(1,3))
halfnorm(residuals(glm_CDA_final,"pearson"))
plot(glm_CDA_final, which = 1, id.n = 5)
plot(glm_CDA_final, which = 4, id.n = 5)
```
There is still a large spread of both the pearson (left plot) and deviance (middle plot) residuals. Furthermore, there is non-constant errror variance. The cook's distance does not display very influential observations.

```{r}
vif(glm_CDA_final)
avPlots(glm_CDA_final)
```
After removing two outliers, the partial regression coefficients represent the data much better. There are no strong correlations between the explanatory variables and the respons. The low VIF values also indicate this.


```{r}
function_nonwest1 <- function(x){
  -1.09  - 0.14*x -1.23*x
}

function_nonwest2 <- function(x){
  -1.09  - 0.14*x -1.23*x - 0.12*x
}

function_nonwest3 <- function(x){
  -1.09  - 0.14*x -1.23*x - 0.34*x
}

plot(function_nonwest1(1:10), type='l')
lines(function_nonwest2(1:10), type="l", col = "red")
lines(function_nonwest3(1:10), type="l", col = "blue")
```

## Cross validation

```{r}
index <- rep(1:K, floor(nrow(Data_CDA)/K)+1)[1:nrow(Data_CDA)]
summary(as.factor(index))
fold.index <- sample(index)


loss <- numeric(K)

for (k in 1:K){
  training <- Data_CDA[fold.index!=k, ]
  validation <- Data_CDA[fold.index==k, ]
  training.fit <- glm_CDA_final
  validation.predict <- predict(training.fit, newdata=validation, type='response')
  loss[k] <- Loss(validation$CDA_frac, validation.predict)
}


#average, with weights equal to the number of objects used to calculate 
# the loss at each fold:
mean(loss)
```


 

# 4. Discussion

## Limitations 

# 5. Conclusion